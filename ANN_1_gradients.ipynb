{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Gradients\n",
    "<a><strong><tt>autograd</tt></strong></a> implementation of gradient descent. \n",
    "* <a><tt><strong>torch.autograd.backward()</strong></tt></a>\n",
    "* <a ><tt><strong>torch.autograd.grad()</strong></tt></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd - Automatic Differentiation\n",
    "\n",
    "The PyTorch <a><strong><tt>autograd</tt></strong></a> package provides automatic differentiation for all operations on Tensors. \n",
    "This is because operations become attributes of the tensors themselves. \n",
    "\n",
    "When a Tensors <tt>.requires_grad</tt> attribute is set to True, it starts to track all operations on it.\n",
    "\n",
    "When an operation finishes you can call <tt>.backward()</tt> and have all the gradients computed automatically.\n",
    "\n",
    "The gradient for a tensor will be accumulated into its <tt>.grad</tt> attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <a><u> Back-Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "#create a tensor to track all the operations on it\n",
    "x=torch.tensor(2.0,requires_grad=True)\n",
    "\n",
    "#float value is necessary [2(int)] won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# STEP 3\n",
    "#Defined a function\n",
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The output is corresponding to value of y at x=2.0 passed above.\n",
    "###### <a>Since $y$ was created as a result of an operation, it has an associated gradient function accessible as <tt>y.grad_fn</tt><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4\n",
    "# Backdrop\n",
    "\"\"\"Computes the gradient of current tensor\"\"\"\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93.)\n"
     ]
    }
   ],
   "source": [
    "# STEP 5\n",
    "#Display the resulting gradient\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7> <tt>x.grad</tt> is an attribute of tensor $x$, so we don't use parentheses.\n",
    "\n",
    "<h7> Slope of the polynomial at the point(2,63) =93 </h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <a><u>  Back-propagation on multiple steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [3., 2., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1\n",
    "# create tensor\n",
    "x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad= True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  8., 11.],\n",
      "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# STEP 2\n",
    "# Create the first layer with y = 3x+2\n",
    "y = 3*x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# STEP 3\n",
    "# Create the second layer with z = 2y^2\n",
    "z = 2*y**2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(140., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# STEP 4\n",
    "# Set the output to be the matrix mean\n",
    "out = z.mean()\n",
    "print(out)\n",
    "\n",
    "# General mean of all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 16., 22.],\n",
      "        [22., 16., 10.]])\n"
     ]
    }
   ],
   "source": [
    "# STEP 5\n",
    "# Now perform back-propagation to find the gradient of x w.r.t out\n",
    "\n",
    "out.backward()\n",
    "\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a 2x3 matrix. If we call the final <tt>out</tt> tensor \"$o$\", we can calculate the partial derivative of $o$ with respect to $x_i$ as follows:<br>\n",
    "\n",
    "$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$<br>\n",
    "\n",
    "$z_i = 2(y_i)^2 = 2(3x_i+2)^2$<br>\n",
    "\n",
    "To solve the derivative of $z_i$ we use the <a>chain rule</a>, where the derivative of $f(g(x)) = f'(g(x))g'(x)$<br>\n",
    "\n",
    "Therefore,<br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$<br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <a><u>  Turn off tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There may be times when we don't want or need to track the computational history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### You can reset a tensor's <tt>requires_grad</tt> attribute in-place using `.requires_grad_(True)` (or False) as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### When performing evaluations, it's often helpful to wrap a set of operations in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A less-used method is to run `.detach()` on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
